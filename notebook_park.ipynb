{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "186a505e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e9dd40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Projet Spark\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b06160d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema1 = StructType([StructField('Invoice_ID', StringType(), True),\n",
    "                      StructField('Branch', StringType(), True),\n",
    "                      StructField('City', StringType(), True),\n",
    "                      StructField('Customer_type', StringType(), True),\n",
    "                      StructField('Gender', StringType(), True),\n",
    "                      StructField('Product_line', StringType(), True),\n",
    "                      StructField('Unit_price', DoubleType(), True),\n",
    "                      StructField('Quantity', DoubleType(), True),\n",
    "                      StructField('Tax', DoubleType(), True),\n",
    "                      StructField('Total', DoubleType(), True),\n",
    "                      StructField('Date', StringType(), True),\n",
    "                      StructField('Time', StringType(), True),\n",
    "                      StructField('Payment', StringType(), True),\n",
    "                      StructField('cogs', DoubleType(), True),\n",
    "                      StructField('gross_margin_percentage', DoubleType(), True),\n",
    "                      StructField('gross_income', DoubleType(), True),\n",
    "                      StructField('Rating', DoubleType(), True),  \n",
    "                      StructField('month_day', StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db0119e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = spark.read.format('csv')\\\n",
    "                        .schema(schema1)\\\n",
    "                        .option('header', True)\\\n",
    "                        .option(\"delimiter\",\",\")\\\n",
    "                        .load(r'supermarket_sales.csv')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01a062d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = sales.withColumn(\"month_day\",concat_ws(\"-\",split(sales[\"Date\"], '/').getItem(0), split(sales[\"Date\"], '/').getItem(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bee8b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_dates = sales.select(\"month_day\").distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eb716c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_streaming = spark.readStream.format('csv')\\\n",
    "    .schema(schema1)\\\n",
    "    .option(\"recursiveFileLookup\",\"true\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .csv(\"sales_dates/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "24417d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark is streaming  True\n"
     ]
    }
   ],
   "source": [
    "print(\"spark is streaming \" , sales_streaming.isStreaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "50049c76",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Column 'month_day' does not exist. Did you mean one of the following? [Total];\n'Aggregate ['month_day], ['month_day, sum(Total#1098) AS sum(Total)#1129]\n+- Project [Total#1098]\n   +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@70cd0573,csv,List(),Some(StructType(StructField(Invoice_ID,StringType,true),StructField(Branch,StringType,true),StructField(City,StringType,true),StructField(Customer_type,StringType,true),StructField(Gender,StringType,true),StructField(Product_line,StringType,true),StructField(Unit_price,DoubleType,true),StructField(Quantity,DoubleType,true),StructField(Tax,DoubleType,true),StructField(Total,DoubleType,true),StructField(Date,StringType,true),StructField(Time,StringType,true),StructField(Payment,StringType,true),StructField(cogs,DoubleType,true),StructField(gross_margin_percentage,DoubleType,true),StructField(gross_income,DoubleType,true),StructField(Rating,DoubleType,true),StructField(month_day,StringType,true))),List(),None,Map(recursiveFileLookup -> true, header -> true, path -> sales_dates/*.csv),None), FileSource[sales_dates/*.csv], [Invoice_ID#1089, Branch#1090, City#1091, Customer_type#1092, Gender#1093, Product_line#1094, Unit_price#1095, Quantity#1096, Tax#1097, Total#1098, Date#1099, Time#1100, Payment#1101, cogs#1102, gross_margin_percentage#1103, gross_income#1104, Rating#1105, month_day#1106]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-e04ea4afb909>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m maxPurchasePerHour = sales_streaming.selectExpr(\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;34m\"Total\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m       \u001b[0mcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"month_day\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     ).sum(\"Total\")\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\group.py\u001b[0m in \u001b[0;36m_api\u001b[1;34m(self, *cols)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_api\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"GroupedData\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jgd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    194\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Column 'month_day' does not exist. Did you mean one of the following? [Total];\n'Aggregate ['month_day], ['month_day, sum(Total#1098) AS sum(Total)#1129]\n+- Project [Total#1098]\n   +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@70cd0573,csv,List(),Some(StructType(StructField(Invoice_ID,StringType,true),StructField(Branch,StringType,true),StructField(City,StringType,true),StructField(Customer_type,StringType,true),StructField(Gender,StringType,true),StructField(Product_line,StringType,true),StructField(Unit_price,DoubleType,true),StructField(Quantity,DoubleType,true),StructField(Tax,DoubleType,true),StructField(Total,DoubleType,true),StructField(Date,StringType,true),StructField(Time,StringType,true),StructField(Payment,StringType,true),StructField(cogs,DoubleType,true),StructField(gross_margin_percentage,DoubleType,true),StructField(gross_income,DoubleType,true),StructField(Rating,DoubleType,true),StructField(month_day,StringType,true))),List(),None,Map(recursiveFileLookup -> true, header -> true, path -> sales_dates/*.csv),None), FileSource[sales_dates/*.csv], [Invoice_ID#1089, Branch#1090, City#1091, Customer_type#1092, Gender#1093, Product_line#1094, Unit_price#1095, Quantity#1096, Tax#1097, Total#1098, Date#1099, Time#1100, Payment#1101, cogs#1102, gross_margin_percentage#1103, gross_income#1104, Rating#1105, month_day#1106]\n"
     ]
    }
   ],
   "source": [
    "maxPurchasePerHour = sales_streaming.selectExpr(\n",
    "    \"Total\").groupBy(\n",
    "      col(\"month_day\"),\n",
    "    ).sum(\"Total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "678b40d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GroupedData' object has no attribute 'writeStream'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-e7bf31d5302f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmaxPurchasePerHour\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriteStream\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"memory\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"customer_table\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0moutputMode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"complete\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'GroupedData' object has no attribute 'writeStream'"
     ]
    }
   ],
   "source": [
    "maxPurchasePerHour.writeStream\\\n",
    "    .format(\"memory\")\\\n",
    "    .queryName(\"customer_table\")\\\n",
    "    .outputMode(\"complete\")\\\n",
    "    .start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
